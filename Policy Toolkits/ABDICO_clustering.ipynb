{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nvm8jynE0Uaa"
      },
      "source": [
        "# **Welcome to the ABDICO clustering Notebook**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NqA6GhS-SCX5"
      },
      "source": [
        "* This notebook helps recognize dominant groups of related actors, objects, regulated actions (\"aims\") and modals (\"deontics\") in institutional statements.\n",
        "Compared to traditional word based topic modeling such as LDA, we use BERTopic, which pursues a semantic ('word meaning') based approach.\n",
        "\n",
        "### **This notebook performs the following tasks**\n",
        "\n",
        "\n",
        "* It takes a main.csv file which has columns designated  for ABDI components as \"Attribute\", \"Object\", \"Deontic\" and \"Aim\" respectively\n",
        "* User indicates the institutional constituent (ABDICO) over which clustering is to be performed\n",
        "* Output main.csv contains a \"_group\" column, indicating the respective topic cluster to which the constituent belongs.\n",
        "* The categorical topic of the group is indicated by the top most representative words from the cluster.\n",
        "\n",
        "\n",
        "**Note: ** If you are using output files from ABDICO_parsing.ipynb, please make sure to change '_Inf' fields in the csv to respective ABDICO component names. <br/>\n",
        "E.g. 'Attribute_Inf' or inferred Attribute to \"Attribute\" only.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "guL99ntladnA"
      },
      "source": [
        "# **Installations and Setup**\n",
        "* This code sets up the analysis. You don't have to understand it. Just run it and then scroll down.\n",
        "* These commands below install the necessary components for the rest of the analysis to work. To run press ***ctrl+enter*** keys or select ***Runtime*** from the menu above and then one of the ***Run*** options within it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i0-5Tic8-fWm",
        "outputId": "ed520db0-0775-480e-8aec-1d772c8809ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'NLP4GOV'...\n",
            "remote: Enumerating objects: 2310, done.\u001b[K\n",
            "remote: Counting objects: 100% (466/466), done.\u001b[K\n",
            "remote: Compressing objects: 100% (226/226), done.\u001b[K\n",
            "remote: Total 2310 (delta 265), reused 438 (delta 237), pack-reused 1844\u001b[K\n",
            "Receiving objects: 100% (2310/2310), 9.68 MiB | 15.94 MiB/s, done.\n",
            "Resolving deltas: 100% (1364/1364), done.\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m171.5/171.5 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.1/154.1 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m933.2/933.2 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.7/85.7 kB\u001b[0m \u001b[31m564.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m433.8/433.8 kB\u001b[0m \u001b[31m49.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/BSAkash/NLP4GOV\n",
        "!pip install -q -r  ./NLP4GOV/src/ABDICO_clustering/requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C19z3gC5eJwz"
      },
      "source": [
        "# **Data upload**\n",
        "\n",
        "Run this cell to run your own search engine.\n",
        "\n",
        "For your own data, you will likely have to adapt it for this notebook to run. See below for the sample format\n",
        "\n",
        "Please name uploaded files as `main.csv`. For each institutional statement, there must be columns indicating the corresponding ABDICO codings. That is \"Attribute\", \"Object\", \"Deontic\", \"Aim\". These may be hancoded or computationally extracted.\n",
        "\n",
        "See ABDICO_parsing.ipynb in this repository for automated institutional grammar coding of policies.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 38
        },
        "id": "ewLAJqd1rwD_",
        "outputId": "f557d22f-ece4-4774-d8c6-9cb0a8771478"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-983b430e-67fe-4e2d-a6bf-2772f548a616\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-983b430e-67fe-4e2d-a6bf-2772f548a616\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import os\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "os.rename(list(uploaded.keys())[0], 'main.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dwgo6YiaVf_"
      },
      "source": [
        "## **Or use our data archives**\n",
        "\n",
        "You may also uncomment and run the cell below to follow the demonstration on archival data. This shall directly download (into Colab) our datasets for the Apache Software Foundation, an open source software community.\n",
        "\n",
        "* [ABDICO coded Apache community policies](https://storage.googleapis.com/public_data_c2/IG_datasets/ASF_ABDICO.csv)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DTD6MUczTDqF"
      },
      "outputs": [],
      "source": [
        "##Else you can directly use the !wget command below to download our datasets into the code notebook,\n",
        "##make sure you uncomment the below code by (ctrl+/) keys before running this cell of code\n",
        "\n",
        "# # ABDICO coded policy example\n",
        "# !wget -O main.csv https://storage.googleapis.com/public_data_c2/IG_datasets/ASF_ABDICO.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UpUZyDO416cp"
      },
      "source": [
        "# **Clustering and topic modeling**\n",
        "\n",
        "* Prior to uploading the your own dataset remove all the NaN('nan' in python represents missing or undefined data value typically something that is not a number) text value rows from the csv file.\n",
        "* You may need to make some changes to the first three lines of the following cell:\n",
        "\n",
        "  * **Component** : The insitutional component of policies/institutional statements you would like to cluster and group. This should exactly match the column in which data which contains the ABDICO component of interest. Case Sensitive.\n",
        "  * **top_n_words** : The N dominant words to indidate/represent the topic of the group. We recommend setting this number no more than 10.\n",
        "  * **num_topic** : Now this one is a little tricky. This value guides the clustering alogithm to find the specified number of clusters and topics among the ABDICO components. You may use an estimate (e.g. if you are clustering attributes and are aware that there are around 5 different actors in your policy data), or set a slightly higher number. It's simpler to manually combine some similar clusters. Our default is 20, for our apache policies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Klyurgila72n"
      },
      "outputs": [],
      "source": [
        "Component = \"Aim\"\n",
        "top_n_words = 3\n",
        "num_topic = 20\n",
        "\n",
        "# clustering of components\n",
        "from bertopic import BERTopic\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "result = pd.read_csv('main.csv', usecols=[Component])\n",
        "result.dropna(subset=[Component],inplace=True)\n",
        "components = result[Component].tolist()\n",
        "\n",
        "topic_model = BERTopic(top_n_words = 5,nr_topics = num_topic)\n",
        "topic_model.hdbscan_model.gen_min_span_tree=True\n",
        "topic_model.umap_model.random_state= 0 ##set seed to enable reproduction of clustering\n",
        "\n",
        "topic_model.fit(components)\n",
        "freq = topic_model.get_topic_info()\n",
        "result[Component + '_group'] = topic_model.transform(components)[0]\n",
        "topic_index = np.sort(freq['Topic'].values)\n",
        "result[Component + '_group'] = result[Component + '_group'].apply(lambda x : freq.loc[np.argwhere(topic_index==x).squeeze(-1)[0],'Name'])\n",
        "result.to_csv(f'main_{Component}.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "gGqt9tprFUgZ",
        "outputId": "43caeea0-4c44-4fd7-cc58-d7bc61ea20d6"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_efcfa47f-0e61-4d98-8699-dc683bef109c\", \"main_Object.csv\", 18142)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "files.download(f'main_{Component}.csv')#download file here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uv1QzjOvdAe3"
      },
      "source": [
        "### Going deeper\n",
        "Check out [BERTopic](https://maartengr.github.io/BERTopic/) documentation for more arguments, parameters and methods for sophisticated topic modeling. For \"fine tuning\" your topic modeling, see our [work](https://arxiv.org/abs/2309.14245) on governance of open source software."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
